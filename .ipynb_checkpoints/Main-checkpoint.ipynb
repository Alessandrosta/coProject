{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9ce130-ec9f-46e2-b00f-365b170c9c17",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d64a833-5a30-4bbb-a8b2-de258aee51ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model, datasets\n",
    "import os\n",
    "import simplejson\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "\n",
    "import methods\n",
    "import mathFunctions\n",
    "import loadData\n",
    "import makePlots\n",
    "import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612755a0-8299-49b7-bd9e-490dfe8a63cc",
   "metadata": {},
   "source": [
    "### Setup parameters for SN and SCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3061ca8-df40-42e9-a62c-1367352e2df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = 'a9a'  ## Options: a9a, ijcnn1\n",
    "loss = 'Regularised logistic loss'  ## Logistic loss, Regularised logistic loss\n",
    "\n",
    "\n",
    "\n",
    "repetitions = 2\n",
    "learning_rate = 0.5\n",
    "epochs = 50\n",
    "epsilon = 1e-4        ## Smallest possible loss variance\n",
    "minibatch_size = 1000\n",
    "M = 0.02              ## Upper bound for Hessian's Lipschitz constant\n",
    "lambd = 0.001         ## Regularisation parameter of r(w)\n",
    "alpha = 1             ## Parameter of r(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b8f5d-a24d-4418-9607-4b0fd4b94ffc",
   "metadata": {},
   "source": [
    "### Setup parameters for SCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751603eb-06d4-4f48-9c16-e87c73b6b304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following parameters are optional in the sense that default values are set if not specified.\n",
    "opt = {}\n",
    "\n",
    "### TR and SCR:\n",
    "opt['penalty_increase_multiplier']=2.    # multiply by..\n",
    "opt['penalty_derease_multiplier']=2.     # divide by..\n",
    "opt['initial_penalty_parameter']=0.01\n",
    "opt['initial_tr_radius']=1\n",
    "opt['successful_treshold']=0.1\n",
    "opt['very_successful_treshold']=0.9\n",
    "\n",
    "opt['grad_tol']=1e-9\n",
    "opt['n_iterations'] = epochs\n",
    "\n",
    "# Sampling\n",
    "opt['Hessian_sampling']=True\n",
    "opt['gradient_sampling']=True\n",
    "opt['initial_sample_size_Hessian']=0.025\n",
    "opt['initial_sample_size_gradient']=0.05\n",
    "opt['sampling_scheme'] = 'linear'       ## exponential, linear, adaptive\n",
    "opt['subproblem_solver']='cauchy_point'\n",
    "opt['unsuccessful_sample_scaling']=1.5\n",
    "opt['sample_scaling_Hessian']=1\n",
    "opt['sample_scaling_gradient']=1\n",
    "\n",
    "# Subproblem \n",
    "opt['subproblem_solver_SCR']='cauchy_point' # alternatives: lanczos, cauchy_point, exact\n",
    "#opt['subproblem_solver_TR']='GLTR' # alternatives: GLTR, cauchy_point, exact, dog_leg, cg\n",
    "\n",
    "opt['solve_each_i-th_krylov_space']=1   \n",
    "opt['krylov_tol']=1e-1\n",
    "opt['exact_tol']=1e-2\n",
    "opt['keep_Q_matrix_in_memory']=True\n",
    "\n",
    "# reg loss parameters\n",
    "opt['lambda'] = lambd\n",
    "opt['alpha'] = alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d8538-3320-4b03-a6dc-1fc79ac42172",
   "metadata": {},
   "source": [
    "### PLEASE WORK I BEG YOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38535527-a3e9-4c9d-8659-fd814dade453",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Subsampled Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: cauchy_point\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: True\n",
      "- Sampling_scheme: linear \n",
      "\n",
      "Accuracy epoch 0: 0.7591904425539756\n",
      "Iteration 0: loss = 0.5267895041102914 norm_grad = 0.697690888226593 time=  0.01 penalty= 0.005 stepnorm= 0.4868663254938274 Samples Hessian= 814 samples Gradient= 1628 \n",
      "\n",
      "Iteration 1: loss = 0.451030409015717 norm_grad = 0.2186291266313338 time=  0.018 penalty= 0.005 stepnorm= 1.135031896939154 Samples Hessian= 1302 samples Gradient= 1628 \n",
      "\n",
      "Iteration 2: loss = 0.39590642976299995 norm_grad = 0.34709944246478586 time=  0.021 penalty= 0.0025 stepnorm= 0.30299377847962844 Samples Hessian= 1953 samples Gradient= 1953 \n",
      "\n",
      "Iteration 3: loss = 0.38414978627578555 norm_grad = 0.06906928624389586 time=  0.024 penalty= 0.0025 stepnorm= 0.40164113742353585 Samples Hessian= 2604 samples Gradient= 2604 \n",
      "\n",
      "Iteration 4: loss = 0.3751419695327099 norm_grad = 0.1309373271172858 time=  0.02 penalty= 0.0025 stepnorm= 0.1685171616693973 Samples Hessian= 3256 samples Gradient= 3256 \n",
      "\n",
      "Iteration 5: loss = 0.3676906456615679 norm_grad = 0.04555947429600638 time=  0.021 penalty= 0.00125 stepnorm= 0.2863115196270916 Samples Hessian= 3907 samples Gradient= 3907 \n",
      "\n",
      "Iteration 6: loss = 0.3640547171555719 norm_grad = 0.07135696200938037 time=  0.022 penalty= 0.000625 stepnorm= 0.10999535335220495 Samples Hessian= 4558 samples Gradient= 4558 \n",
      "\n",
      "Iteration 7: loss = 0.36096576788586265 norm_grad = 0.03746544082005606 time=  0.024 penalty= 0.0003125 stepnorm= 0.17229125010438767 Samples Hessian= 5209 samples Gradient= 5209 \n",
      "\n",
      "Iteration 8: loss = 0.3584886068479202 norm_grad = 0.05090167799774791 time=  0.022 penalty= 0.00015625 stepnorm= 0.08633142028968627 Samples Hessian= 5860 samples Gradient= 5860 \n",
      "\n",
      "Iteration 9: loss = 0.3562906876583084 norm_grad = 0.031387380580198986 time=  0.027 penalty= 0.00015625 stepnorm= 0.16598742154045837 Samples Hessian= 6512 samples Gradient= 6512 \n",
      "\n",
      "Iteration 10: loss = 0.3543159247431526 norm_grad = 0.055546613584543644 time=  0.028 penalty= 0.00015625 stepnorm= 0.08250512680230442 Samples Hessian= 7163 samples Gradient= 7163 \n",
      "\n",
      "Iteration 11: loss = 0.3497984926195399 norm_grad = 0.02598141595902813 time=  0.03 penalty= 0.00015625 stepnorm= 0.43657839898302797 Samples Hessian= 7814 samples Gradient= 7814 \n",
      "\n",
      "Iteration 12: loss = 0.3475267459573746 norm_grad = 0.06441399313329801 time=  0.033 penalty= 0.00015625 stepnorm= 0.08707750200975589 Samples Hessian= 8465 samples Gradient= 8465 \n",
      "\n",
      "Iteration 13: loss = 0.34674082080481217 norm_grad = 0.020238737835856155 time=  0.029 penalty= 0.00015625 stepnorm= 0.09144153460678742 Samples Hessian= 9117 samples Gradient= 9117 \n",
      "\n",
      "Iteration 14: loss = 0.3458122165940119 norm_grad = 0.023391534852411574 time=  0.032 penalty= 7.8125e-05 stepnorm= 0.06161043790952144 Samples Hessian= 9768 samples Gradient= 9768 \n",
      "\n",
      "Iteration 15: loss = 0.34516878722202554 norm_grad = 0.018552466334357694 time=  0.03 penalty= 7.8125e-05 stepnorm= 0.09045077837507244 Samples Hessian= 10419 samples Gradient= 10419 \n",
      "\n",
      "Iteration 16: loss = 0.3444897576507965 norm_grad = 0.025617754457582232 time=  0.034 penalty= 3.90625e-05 stepnorm= 0.046217160279552215 Samples Hessian= 11070 samples Gradient= 11070 \n",
      "\n",
      "Iteration 17: loss = 0.34441857587030483 norm_grad = 0.0205420506302222 time=  0.035 penalty= 3.90625e-05 stepnorm= 0.04651589473103189 Samples Hessian= 11721 samples Gradient= 11721 \n",
      "\n",
      "Iteration 18: loss = 0.34386896173016185 norm_grad = 0.024559545672673645 time=  0.034 penalty= 1.953125e-05 stepnorm= 0.043976413509612566 Samples Hessian= 12373 samples Gradient= 12373 \n",
      "\n",
      "Iteration 19: loss = 0.34329575411202606 norm_grad = 0.015387586837366699 time=  0.032 penalty= 1.953125e-05 stepnorm= 0.09533843823516157 Samples Hessian= 13024 samples Gradient= 13024 \n",
      "\n",
      "Iteration 20: loss = 0.3426610435727554 norm_grad = 0.024730145605293852 time=  0.038 penalty= 9.765625e-06 stepnorm= 0.045712624559683515 Samples Hessian= 13675 samples Gradient= 13675 \n",
      "\n",
      "Iteration 21: loss = 0.34260646130948086 norm_grad = 0.01664720599194664 time=  0.038 penalty= 9.765625e-06 stepnorm= 0.04092028312596556 Samples Hessian= 14326 samples Gradient= 14326 \n",
      "\n",
      "Iteration 22: loss = 0.34231227279363075 norm_grad = 0.027143735562709527 time=  0.036 penalty= 9.765625e-06 stepnorm= 0.041710408473645974 Samples Hessian= 14978 samples Gradient= 14978 \n",
      "\n",
      "Iteration 23: loss = 0.3422279527526908 norm_grad = 0.026075528721360885 time=  0.04 penalty= 9.765625e-06 stepnorm= 0.04017934043977423 Samples Hessian= 15629 samples Gradient= 15629 \n",
      "\n",
      "Iteration 24: loss = 0.3419692140254161 norm_grad = 0.02532741603784096 time=  0.036 penalty= 9.765625e-06 stepnorm= 0.04065446680150736 Samples Hessian= 16280 samples Gradient= 16280 \n",
      "\n",
      "Iteration 25: loss = 0.3417381243674423 norm_grad = 0.016823134533683292 time=  0.039 penalty= 9.765625e-06 stepnorm= 0.03776620660653143 Samples Hessian= 16931 samples Gradient= 16931 \n",
      "\n",
      "Iteration 26: loss = 0.3409889858154953 norm_grad = 0.011470368802926241 time=  0.037 penalty= 4.8828125e-06 stepnorm= 0.1204231740628761 Samples Hessian= 17582 samples Gradient= 17582 \n",
      "\n",
      "Iteration 27: loss = 0.34069981717902226 norm_grad = 0.030971686480002984 time=  0.041 penalty= 4.8828125e-06 stepnorm= 0.043234748604591124 Samples Hessian= 18234 samples Gradient= 18234 \n",
      "\n",
      "Iteration 28: loss = 0.34055384113387516 norm_grad = 0.01862879104032677 time=  0.043 penalty= 4.8828125e-06 stepnorm= 0.031468754282436624 Samples Hessian= 18885 samples Gradient= 18885 \n",
      "\n",
      "Iteration 29: loss = 0.34036699918231555 norm_grad = 0.014960953152721626 time=  0.042 penalty= 4.8828125e-06 stepnorm= 0.03729846490758381 Samples Hessian= 19536 samples Gradient= 19536 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 30: loss = 0.34036699918231555 norm_grad = 0.027552620323456536 time=  0.04 penalty= 9.765625e-06 stepnorm= 0.04059430723091176 Samples Hessian= 20187 samples Gradient= 20187 \n",
      "\n",
      "Iteration 31: loss = 0.3402181735648563 norm_grad = 0.01585023681104266 time=  0.041 penalty= 9.765625e-06 stepnorm= 0.02892709186130659 Samples Hessian= 20839 samples Gradient= 20839 \n",
      "\n",
      "Iteration 32: loss = 0.34000117683372294 norm_grad = 0.011678469806508636 time=  0.045 penalty= 9.765625e-06 stepnorm= 0.04347375525552278 Samples Hessian= 21490 samples Gradient= 21490 \n",
      "\n",
      "Iteration 33: loss = 0.3398404940243362 norm_grad = 0.014975256766090067 time=  0.043 penalty= 9.765625e-06 stepnorm= 0.02842371487734839 Samples Hessian= 22141 samples Gradient= 22141 \n",
      "\n",
      "Iteration 34: loss = 0.33960242297519655 norm_grad = 0.010076684177785372 time=  0.041 penalty= 9.765625e-06 stepnorm= 0.059381661251309864 Samples Hessian= 22792 samples Gradient= 22792 \n",
      "\n",
      "Iteration 35: loss = 0.3393855918248037 norm_grad = 0.014441824847968954 time=  0.048 penalty= 4.8828125e-06 stepnorm= 0.030696144187503736 Samples Hessian= 23443 samples Gradient= 23443 \n",
      "\n",
      "Iteration 36: loss = 0.33926392475714784 norm_grad = 0.011084346849654704 time=  0.048 penalty= 4.8828125e-06 stepnorm= 0.060631598253126875 Samples Hessian= 24095 samples Gradient= 24095 \n",
      "\n",
      "Iteration 37: loss = 0.3390788726230176 norm_grad = 0.02434160908694366 time=  0.043 penalty= 4.8828125e-06 stepnorm= 0.034660608411566436 Samples Hessian= 24746 samples Gradient= 24746 \n",
      "\n",
      "Iteration 38: loss = 0.3389529482580093 norm_grad = 0.011245063175256491 time=  0.044 penalty= 2.44140625e-06 stepnorm= 0.023848455120618985 Samples Hessian= 25397 samples Gradient= 25397 \n",
      "\n",
      "Iteration 39: loss = 0.3382124201123017 norm_grad = 0.008184793854419666 time=  0.046 penalty= 2.44140625e-06 stepnorm= 0.33588254121100236 Samples Hessian= 26048 samples Gradient= 26048 \n",
      "\n",
      "Iteration 40: loss = 0.33792727487434393 norm_grad = 0.01600534088803919 time=  0.045 penalty= 1.220703125e-06 stepnorm= 0.03637595732694399 Samples Hessian= 26700 samples Gradient= 26700 \n",
      "\n",
      "Iteration 41: loss = 0.3378036467731761 norm_grad = 0.012441759779591948 time=  0.045 penalty= 1.220703125e-06 stepnorm= 0.026838355973712733 Samples Hessian= 27351 samples Gradient= 27351 \n",
      "\n",
      "Iteration 42: loss = 0.3374313817487772 norm_grad = 0.009284727476178943 time=  0.046 penalty= 6.103515625e-07 stepnorm= 0.0679247502914318 Samples Hessian= 28002 samples Gradient= 28002 \n",
      "\n",
      "Iteration 43: loss = 0.33727475312639715 norm_grad = 0.009061168785096914 time=  0.046 penalty= 3.0517578125e-07 stepnorm= 0.023263768627113917 Samples Hessian= 28653 samples Gradient= 28653 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 44: loss = 0.33727475312639715 norm_grad = 0.009577411668278883 time=  0.052 penalty= 6.103515625e-07 stepnorm= 0.02499129676572818 Samples Hessian= 29304 samples Gradient= 29304 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 45: loss = 0.33727475312639715 norm_grad = 0.009334108177524484 time=  0.05 penalty= 1.220703125e-06 stepnorm= 0.022445263384726567 Samples Hessian= 29956 samples Gradient= 29956 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 46: loss = 0.33727475312639715 norm_grad = 0.008143321811483794 time=  0.049 penalty= 2.44140625e-06 stepnorm= 0.02291879467438451 Samples Hessian= 30607 samples Gradient= 30607 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 47: loss = 0.33727475312639715 norm_grad = 0.009837460052106506 time=  0.047 penalty= 4.8828125e-06 stepnorm= 0.03148823075800129 Samples Hessian= 31258 samples Gradient= 31258 \n",
      "\n",
      "Iteration 48: loss = 0.3372046190404826 norm_grad = 0.007548532380131148 time=  0.05 penalty= 4.8828125e-06 stepnorm= 0.04142495099366209 Samples Hessian= 31909 samples Gradient= 31909 \n",
      "\n",
      "Iteration 49: loss = 0.33706355046716907 norm_grad = 0.013702777771663215 time=  0.046 penalty= 2.44140625e-06 stepnorm= 0.02065584154540968 Samples Hessian= 32561 samples Gradient= 32561 \n",
      "\n",
      "--- Subsampled Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: cauchy_point\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: True\n",
      "- Sampling_scheme: linear \n",
      "\n",
      "Accuracy epoch 0: 0.7591904425539756\n",
      "Iteration 0: loss = 0.5226048959719323 norm_grad = 0.6410469913431448 time=  0.009 penalty= 0.005 stepnorm= 0.4568369931748835 Samples Hessian= 814 samples Gradient= 1628 \n",
      "\n",
      "Iteration 1: loss = 0.4047992766645041 norm_grad = 0.19341788611599398 time=  0.019 penalty= 0.0025 stepnorm= 1.0154616084331978 Samples Hessian= 1302 samples Gradient= 1628 \n",
      "\n",
      "Iteration 2: loss = 0.3943146302816432 norm_grad = 0.12216625795632428 time=  0.017 penalty= 0.0025 stepnorm= 0.21093458381083804 Samples Hessian= 1953 samples Gradient= 1953 \n",
      "\n",
      "Iteration 3: loss = 0.3887485735200241 norm_grad = 0.09959980712121144 time=  0.023 penalty= 0.0025 stepnorm= 0.13751943441756903 Samples Hessian= 2604 samples Gradient= 2604 \n",
      "\n",
      "Iteration 4: loss = 0.3791245485521994 norm_grad = 0.05794353606624697 time=  0.022 penalty= 0.00125 stepnorm= 0.2701286353656089 Samples Hessian= 3256 samples Gradient= 3256 \n",
      "\n",
      "Iteration 5: loss = 0.3743700084158635 norm_grad = 0.08332339127884046 time=  0.026 penalty= 0.000625 stepnorm= 0.12376914151141404 Samples Hessian= 3907 samples Gradient= 3907 \n",
      "\n",
      "Iteration 6: loss = 0.37111382844477725 norm_grad = 0.053174917508131685 time=  0.021 penalty= 0.000625 stepnorm= 0.15002553190477813 Samples Hessian= 4558 samples Gradient= 4558 \n",
      "\n",
      "Iteration 7: loss = 0.36748571606844016 norm_grad = 0.06284235369328102 time=  0.025 penalty= 0.0003125 stepnorm= 0.10838407891071565 Samples Hessian= 5209 samples Gradient= 5209 \n",
      "\n",
      "Iteration 8: loss = 0.36372115175676684 norm_grad = 0.03987467545651035 time=  0.026 penalty= 0.00015625 stepnorm= 0.20413237772082551 Samples Hessian= 5860 samples Gradient= 5860 \n",
      "\n",
      "Iteration 9: loss = 0.36096948256804456 norm_grad = 0.059010319891329106 time=  0.03 penalty= 7.8125e-05 stepnorm= 0.08394400470789946 Samples Hessian= 6512 samples Gradient= 6512 \n",
      "\n",
      "Iteration 10: loss = 0.35867158231135987 norm_grad = 0.03351026217193959 time=  0.026 penalty= 7.8125e-05 stepnorm= 0.20752693331512986 Samples Hessian= 7163 samples Gradient= 7163 \n",
      "\n",
      "Iteration 11: loss = 0.35577432129620096 norm_grad = 0.06220723813937849 time=  0.031 penalty= 3.90625e-05 stepnorm= 0.08211825436084717 Samples Hessian= 7814 samples Gradient= 7814 \n",
      "\n",
      "Iteration 12: loss = 0.3554292555167197 norm_grad = 0.03320537301777468 time=  0.028 penalty= 3.90625e-05 stepnorm= 0.0994212591085781 Samples Hessian= 8465 samples Gradient= 8465 \n",
      "\n",
      "Iteration 13: loss = 0.3535617419894303 norm_grad = 0.05958448181161061 time=  0.033 penalty= 3.90625e-05 stepnorm= 0.08120716782801932 Samples Hessian= 9117 samples Gradient= 9117 \n",
      "\n",
      "Iteration 14: loss = 0.352502807621301 norm_grad = 0.02734519154797411 time=  0.03 penalty= 3.90625e-05 stepnorm= 0.08860363691823708 Samples Hessian= 9768 samples Gradient= 9768 \n",
      "\n",
      "Iteration 15: loss = 0.35156880823600395 norm_grad = 0.040287263545039496 time=  0.035 penalty= 3.90625e-05 stepnorm= 0.06400239257415458 Samples Hessian= 10419 samples Gradient= 10419 \n",
      "\n",
      "Iteration 16: loss = 0.34997209210363267 norm_grad = 0.02196336505956898 time=  0.037 penalty= 1.953125e-05 stepnorm= 0.11939768813081633 Samples Hessian= 11070 samples Gradient= 11070 \n",
      "\n",
      "Iteration 17: loss = 0.34924320814622317 norm_grad = 0.039978297975011526 time=  0.039 penalty= 1.953125e-05 stepnorm= 0.0592573122393012 Samples Hessian= 11721 samples Gradient= 11721 \n",
      "\n",
      "Iteration 18: loss = 0.3484948191714699 norm_grad = 0.02261630723037523 time=  0.04 penalty= 9.765625e-06 stepnorm= 0.06375709787026985 Samples Hessian= 12373 samples Gradient= 12373 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 19: loss = 0.3484948191714699 norm_grad = 0.0429566658185897 time=  0.039 penalty= 1.953125e-05 stepnorm= 0.05902571207489526 Samples Hessian= 13024 samples Gradient= 13024 \n",
      "\n",
      "Iteration 20: loss = 0.3478626092635772 norm_grad = 0.02473786686941264 time=  0.039 penalty= 9.765625e-06 stepnorm= 0.056235059685237135 Samples Hessian= 13675 samples Gradient= 13675 \n",
      "\n",
      "Iteration 21: loss = 0.3472308844160974 norm_grad = 0.022651289590679118 time=  0.047 penalty= 4.8828125e-06 stepnorm= 0.06148194452907127 Samples Hessian= 14326 samples Gradient= 14326 \n",
      "\n",
      "Iteration 22: loss = 0.34676168319388273 norm_grad = 0.02533397439340319 time=  0.042 penalty= 4.8828125e-06 stepnorm= 0.047088606795578365 Samples Hessian= 14978 samples Gradient= 14978 \n",
      "\n",
      "Iteration 23: loss = 0.3433546524456758 norm_grad = 0.016740662071756902 time=  0.041 penalty= 4.8828125e-06 stepnorm= 0.5027692404706839 Samples Hessian= 15629 samples Gradient= 15629 \n",
      "\n",
      "Iteration 24: loss = 0.3419675829550249 norm_grad = 0.04007573180309699 time=  0.045 penalty= 2.44140625e-06 stepnorm= 0.056242102922961765 Samples Hessian= 16280 samples Gradient= 16280 \n",
      "\n",
      "Iteration 25: loss = 0.340224824185728 norm_grad = 0.014395450818824311 time=  0.045 penalty= 2.44140625e-06 stepnorm= 0.34096863350649254 Samples Hessian= 16931 samples Gradient= 16931 \n",
      "\n",
      "Iteration 26: loss = 0.33979967758376367 norm_grad = 0.03369402003342324 time=  0.047 penalty= 2.44140625e-06 stepnorm= 0.04765038814783887 Samples Hessian= 17582 samples Gradient= 17582 \n",
      "\n",
      "Iteration 27: loss = 0.33960490821662376 norm_grad = 0.017599439049141796 time=  0.046 penalty= 2.44140625e-06 stepnorm= 0.037313184493716914 Samples Hessian= 18234 samples Gradient= 18234 \n",
      "\n",
      "Iteration 28: loss = 0.3390528068452253 norm_grad = 0.011330221598296032 time=  0.051 penalty= 1.220703125e-06 stepnorm= 0.10611859681790672 Samples Hessian= 18885 samples Gradient= 18885 \n",
      "\n",
      "Iteration 29: loss = 0.33889618157089035 norm_grad = 0.02203027468529473 time=  0.047 penalty= 1.220703125e-06 stepnorm= 0.033659719800429234 Samples Hessian= 19536 samples Gradient= 19536 \n",
      "\n",
      "Iteration 30: loss = 0.33877041771816835 norm_grad = 0.012516948371184273 time=  0.046 penalty= 1.220703125e-06 stepnorm= 0.027984148363279074 Samples Hessian= 20187 samples Gradient= 20187 \n",
      "\n",
      "Iteration 31: loss = 0.33868495451736835 norm_grad = 0.012344733077916104 time=  0.05 penalty= 1.220703125e-06 stepnorm= 0.028276155317659115 Samples Hessian= 20839 samples Gradient= 20839 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 32: loss = 0.33868495451736835 norm_grad = 0.009177799647700205 time=  0.049 penalty= 2.44140625e-06 stepnorm= 0.14119889056967114 Samples Hessian= 21490 samples Gradient= 21490 \n",
      "\n",
      "Iteration 33: loss = 0.3386276549888646 norm_grad = 0.01765773676233842 time=  0.047 penalty= 2.44140625e-06 stepnorm= 0.02868182621175036 Samples Hessian= 22141 samples Gradient= 22141 \n",
      "\n",
      "Iteration 34: loss = 0.3384753827361426 norm_grad = 0.011597389069693443 time=  0.049 penalty= 2.44140625e-06 stepnorm= 0.03277298908736678 Samples Hessian= 22792 samples Gradient= 22792 \n",
      "\n",
      "Iteration 35: loss = 0.337822558010744 norm_grad = 0.007404035197322147 time=  0.053 penalty= 2.44140625e-06 stepnorm= 0.45283513538069353 Samples Hessian= 23443 samples Gradient= 23443 \n",
      "\n",
      "Iteration 36: loss = 0.3374098747589147 norm_grad = 0.02055007986587451 time=  0.054 penalty= 1.220703125e-06 stepnorm= 0.03092559688190599 Samples Hessian= 24095 samples Gradient= 24095 \n",
      "\n",
      "Iteration 37: loss = 0.33723158423479027 norm_grad = 0.009970745940298256 time=  0.048 penalty= 1.220703125e-06 stepnorm= 0.09808884476189428 Samples Hessian= 24746 samples Gradient= 24746 \n",
      "\n",
      "Iteration 38: loss = 0.33687766835082905 norm_grad = 0.01972045449404587 time=  0.057 penalty= 6.103515625e-07 stepnorm= 0.028859287168522124 Samples Hessian= 25397 samples Gradient= 25397 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 39: loss = 0.33687766835082905 norm_grad = 0.008613999866926137 time=  0.055 penalty= 1.220703125e-06 stepnorm= 0.03273068443011958 Samples Hessian= 26048 samples Gradient= 26048 \n",
      "\n",
      "Iteration 40: loss = 0.336432874424223 norm_grad = 0.006789588859484445 time=  0.058 penalty= 1.220703125e-06 stepnorm= 0.2849838663610331 Samples Hessian= 26700 samples Gradient= 26700 \n",
      "\n",
      "Iteration 41: loss = 0.3364192943430292 norm_grad = 0.011690272425423774 time=  0.058 penalty= 1.220703125e-06 stepnorm= 0.023217556217506194 Samples Hessian= 27351 samples Gradient= 27351 \n",
      "\n",
      "Iteration 42: loss = 0.336284524521107 norm_grad = 0.010926685802378495 time=  0.056 penalty= 1.220703125e-06 stepnorm= 0.028727262391403528 Samples Hessian= 28002 samples Gradient= 28002 \n",
      "\n",
      "Iteration 43: loss = 0.3362527256477247 norm_grad = 0.010909035664808752 time=  0.052 penalty= 1.220703125e-06 stepnorm= 0.022379908516741286 Samples Hessian= 28653 samples Gradient= 28653 \n",
      "\n",
      "Iteration 44: loss = 0.33610044114230236 norm_grad = 0.007453518447198162 time=  0.057 penalty= 1.220703125e-06 stepnorm= 0.0523977226820326 Samples Hessian= 29304 samples Gradient= 29304 \n",
      "\n",
      "Iteration 45: loss = 0.33605739053876854 norm_grad = 0.011397532851770257 time=  0.057 penalty= 1.220703125e-06 stepnorm= 0.018566259883036576 Samples Hessian= 29956 samples Gradient= 29956 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 46: loss = 0.33605739053876854 norm_grad = 0.013675946404335317 time=  0.059 penalty= 2.44140625e-06 stepnorm= 0.021437354896000443 Samples Hessian= 30607 samples Gradient= 30607 \n",
      "\n",
      "Iteration 47: loss = 0.33600688856815997 norm_grad = 0.0057156766192428795 time=  0.06 penalty= 2.44140625e-06 stepnorm= 0.028211833376461104 Samples Hessian= 31258 samples Gradient= 31258 \n",
      "\n",
      "Iteration 48: loss = 0.33595584122322736 norm_grad = 0.005422058332366997 time=  0.058 penalty= 2.44140625e-06 stepnorm= 0.02596581432520131 Samples Hessian= 31909 samples Gradient= 31909 \n",
      "\n",
      "Iteration 49: loss = 0.3359233959366744 norm_grad = 0.0057402893246126 time=  0.049 penalty= 1.220703125e-06 stepnorm= 0.011324061986314134 Samples Hessian= 32561 samples Gradient= 32561 \n",
      "\n",
      "Accuracy epoch 0: 0.2408095574460244\n",
      "Epoch 1/50, Loss: 0.49251232495700376, Accuracy: 0.8117\n",
      "Epoch 5/50, Loss: 0.35803069032434265, Accuracy: 0.8430\n",
      "Time for the first 10 epochs: 0.8240 seconds\n",
      "Expected total time for 50 epochs: 4.12 seconds\n",
      "Epoch 10/50, Loss: 0.34596837703643274, Accuracy: 0.8440\n",
      "Epoch 15/50, Loss: 0.34596837703643274, Accuracy: 0.8440\n",
      "Epoch 20/50, Loss: 0.34596837703643274, Accuracy: 0.8440\n",
      "Epoch 25/50, Loss: 0.34596837703643274, Accuracy: 0.8440\n",
      "Epoch 30/50, Loss: 0.34476709186148324, Accuracy: 0.8451\n",
      "Epoch 35/50, Loss: 0.34476709186148324, Accuracy: 0.8451\n",
      "Epoch 40/50, Loss: 0.34476709186148324, Accuracy: 0.8451\n",
      "Epoch 45/50, Loss: 0.34476709186148324, Accuracy: 0.8451\n",
      "Epoch 50/50, Loss: 0.34476709186148324, Accuracy: 0.8451\n",
      "total time: 3.91 seconds\n",
      "Accuracy epoch 0: 0.2408095574460244\n",
      "Epoch 1/50, Loss: 0.4972287475937666, Accuracy: 0.7873\n",
      "Epoch 5/50, Loss: 0.3539041091701125, Accuracy: 0.8466\n",
      "Time for the first 10 epochs: 0.7722 seconds\n",
      "Expected total time for 50 epochs: 3.86 seconds\n",
      "Epoch 10/50, Loss: 0.3500168124667533, Accuracy: 0.8435\n",
      "Epoch 15/50, Loss: 0.34998906325623813, Accuracy: 0.8417\n",
      "Epoch 20/50, Loss: 0.34835911628711835, Accuracy: 0.8436\n",
      "Epoch 25/50, Loss: 0.34835911628711835, Accuracy: 0.8436\n",
      "Epoch 30/50, Loss: 0.34835911628711835, Accuracy: 0.8436\n",
      "Epoch 35/50, Loss: 0.34835911628711835, Accuracy: 0.8436\n",
      "Epoch 40/50, Loss: 0.34835911628711835, Accuracy: 0.8436\n",
      "Epoch 45/50, Loss: 0.34835911628711835, Accuracy: 0.8436\n",
      "Epoch 50/50, Loss: 0.34835911628711835, Accuracy: 0.8436\n",
      "total time: 3.86 seconds\n",
      "Accuracy epoch 0: 0.2408095574460244\n",
      "Epoch 1/50, Loss: 0.41707239234866683, Accuracy: 0.7925\n",
      "Epoch 5/50, Loss: 0.3531987690224027, Accuracy: 0.8390\n",
      "Time for the first 10 epochs: 0.9822 seconds\n",
      "Expected total time for 50 epochs: 4.91 seconds\n",
      "Epoch 10/50, Loss: 0.3436520106438337, Accuracy: 0.8428\n",
      "Epoch 15/50, Loss: 0.3421115804937929, Accuracy: 0.8428\n",
      "Epoch 20/50, Loss: 0.34094641185991664, Accuracy: 0.8440\n",
      "Epoch 25/50, Loss: 0.3392785978029371, Accuracy: 0.8443\n",
      "Epoch 30/50, Loss: 0.3392785978029371, Accuracy: 0.8443\n",
      "Epoch 35/50, Loss: 0.3382908309297672, Accuracy: 0.8436\n",
      "Epoch 40/50, Loss: 0.3382312099358795, Accuracy: 0.8443\n",
      "Epoch 45/50, Loss: 0.3377611452490958, Accuracy: 0.8446\n",
      "Epoch 50/50, Loss: 0.3377611452490958, Accuracy: 0.8446\n",
      "total time: 4.80 seconds\n",
      "Accuracy epoch 0: 0.2408095574460244\n",
      "Epoch 1/50, Loss: 0.4330377193208261, Accuracy: 0.7812\n",
      "Epoch 5/50, Loss: 0.3545239991213692, Accuracy: 0.8381\n",
      "Time for the first 10 epochs: 0.8965 seconds\n",
      "Expected total time for 50 epochs: 4.48 seconds\n",
      "Epoch 10/50, Loss: 0.34442305804304774, Accuracy: 0.8429\n",
      "Epoch 15/50, Loss: 0.3418755771447304, Accuracy: 0.8432\n",
      "Epoch 20/50, Loss: 0.3402201522211797, Accuracy: 0.8438\n",
      "Epoch 25/50, Loss: 0.33937151847958913, Accuracy: 0.8449\n",
      "Epoch 30/50, Loss: 0.33899552707801955, Accuracy: 0.8449\n",
      "Epoch 35/50, Loss: 0.33899552707801955, Accuracy: 0.8449\n",
      "Epoch 40/50, Loss: 0.3386018195515525, Accuracy: 0.8444\n",
      "Epoch 45/50, Loss: 0.33856237026800245, Accuracy: 0.8436\n",
      "Epoch 50/50, Loss: 0.3382876473915765, Accuracy: 0.8445\n",
      "total time: 5.04 seconds\n"
     ]
    }
   ],
   "source": [
    "runs = Experiment.execution(dataset_name, loss, repetitions, learning_rate, epochs, epsilon, minibatch_size, M, lambd, alpha, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37ec949b-a74f-404e-b4bc-7936be84201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots\n",
    "makePlots.plot_loss_vs_iterations(runs, epsilon, filename='loss_diff_vs_iterations.png')\n",
    "makePlots.plot_loss_vs_time(runs, epsilon, filename='loss_diff_vs_time.png')\n",
    "makePlots.plot_accuracy_vs_iterations(runs, filename='accuracy_vs_iterations.png')\n",
    "makePlots.plot_accuracy_vs_time(runs, filename='accuracy_vs_time.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
