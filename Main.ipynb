{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9ce130-ec9f-46e2-b00f-365b170c9c17",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d64a833-5a30-4bbb-a8b2-de258aee51ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model, datasets\n",
    "import os\n",
    "import simplejson\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "\n",
    "import methods\n",
    "import mathFunctions\n",
    "import loadData\n",
    "import makePlots\n",
    "import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612755a0-8299-49b7-bd9e-490dfe8a63cc",
   "metadata": {},
   "source": [
    "### Setup parameters for SN and SCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3061ca8-df40-42e9-a62c-1367352e2df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = 'ijcnn1'  ## Options: a9a, ijcnn1\n",
    "loss = 'Regularised logistic loss'  ## Logistic loss, Regularised logistic loss\n",
    "repetitions = 5\n",
    "learning_rate = 0.5\n",
    "epochs = 30\n",
    "epsilon = 1e-4        ## Smallest possible loss variance\n",
    "minibatch_size = 128\n",
    "M = 0.02              ## Upper bound for Hessian's Lipschitz constant\n",
    "lambd = 0.001         ## Regularisation parameter of r(w)\n",
    "alpha = 1             ## Parameter of r(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b8f5d-a24d-4418-9607-4b0fd4b94ffc",
   "metadata": {},
   "source": [
    "### Setup parameters for SCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751603eb-06d4-4f48-9c16-e87c73b6b304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following parameters are optional in the sense that default values are set if not specified.\n",
    "opt = {}\n",
    "\n",
    "### TR and SCR:\n",
    "opt['penalty_increase_multiplier']=2.    # multiply by..\n",
    "opt['penalty_derease_multiplier']=2.     # divide by..\n",
    "opt['initial_penalty_parameter']=0.01\n",
    "opt['initial_tr_radius']=1\n",
    "opt['successful_treshold']=0.1\n",
    "opt['very_successful_treshold']=0.9\n",
    "\n",
    "opt['grad_tol']=1e-9\n",
    "opt['n_iterations'] = epochs\n",
    "\n",
    "# Sampling\n",
    "opt['Hessian_sampling']=True\n",
    "opt['gradient_sampling']=True\n",
    "opt['initial_sample_size_Hessian']=0.025\n",
    "opt['initial_sample_size_gradient']=0.05\n",
    "opt['sampling_scheme'] = 'linear'       ## exponential, linear, adaptive\n",
    "opt['subproblem_solver']='cauchy_point'\n",
    "opt['unsuccessful_sample_scaling']=1.5\n",
    "opt['sample_scaling_Hessian']=1\n",
    "opt['sample_scaling_gradient']=1\n",
    "\n",
    "# Subproblem \n",
    "opt['subproblem_solver_SCR']='cauchy_point' # alternatives: lanczos, cauchy_point, exact\n",
    "#opt['subproblem_solver_TR']='GLTR' # alternatives: GLTR, cauchy_point, exact, dog_leg, cg\n",
    "\n",
    "opt['solve_each_i-th_krylov_space']=1   \n",
    "opt['krylov_tol']=1e-1\n",
    "opt['exact_tol']=1e-2\n",
    "opt['keep_Q_matrix_in_memory']=True\n",
    "\n",
    "# reg loss parameters\n",
    "opt['lambda'] = lambd\n",
    "opt['alpha'] = alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d8538-3320-4b03-a6dc-1fc79ac42172",
   "metadata": {},
   "source": [
    "### Compare Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38535527-a3e9-4c9d-8659-fd814dade453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = 'a9a'  ## Options: a9a, ijcnn1\n",
    "loss = 'Regularised logistic loss'  ## Logistic loss, Regularised logistic loss\n",
    "repetitions = 3\n",
    "learning_rate = 0.5\n",
    "epochs = 30\n",
    "epsilon = 1e-4        ## Smallest possible loss variance\n",
    "minibatch_size = 128\n",
    "M = 0.02              ## Upper bound for Hessian's Lipschitz constant\n",
    "lambd = 0.001         ## Regularisation parameter of r(w)\n",
    "alpha = 1             ## Parameter of r(w)\n",
    "\n",
    "runs = Experiment.compare_Methods(dataset_name, loss, repetitions, learning_rate, epochs, minibatch_size, M, lambd, alpha, opt)\n",
    "# Generate plots\n",
    "makePlots.plot_loss_vs_iterations(runs, epsilon, filename='Methods_LvI.png')\n",
    "makePlots.plot_loss_vs_time(runs, epsilon, filename='Methods_LvT.png')\n",
    "makePlots.plot_accuracy_vs_iterations(runs, filename='Methods_AvI.png')\n",
    "makePlots.plot_accuracy_vs_time(runs, filename='Methods_AvT.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b06c1b",
   "metadata": {},
   "source": [
    "### Compare Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec949b-a74f-404e-b4bc-7936be84201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'a9a'  ## Options: a9a, ijcnn1\n",
    "repetitions = 3\n",
    "learning_rate = 0.5\n",
    "epochs = 30\n",
    "epsilon = 1e-4        ## Smallest possible loss variance\n",
    "minibatch_size = 128\n",
    "M = 0.02              ## Upper bound for Hessian's Lipschitz constant\n",
    "lambd = 0.001         ## Regularisation parameter of r(w)\n",
    "alpha = 1             ## Parameter of r(w)\n",
    "\n",
    "runs = Experiment.compare_Losses(dataset_name, repetitions, learning_rate, epochs, minibatch_size, M, lambd, alpha, opt)\n",
    "# Generate plots\n",
    "# Generate plots\n",
    "makePlots.plot_loss_vs_iterations(runs, epsilon, filename='Losses_LvI.png')\n",
    "makePlots.plot_loss_vs_time(runs, epsilon, filename='Losses_LvT.png')\n",
    "makePlots.plot_accuracy_vs_iterations(runs, filename='Losses_AvI.png')\n",
    "makePlots.plot_accuracy_vs_time(runs, filename='Losses_AvT.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411e60c1",
   "metadata": {},
   "source": [
    "### Compare M Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b148929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ijcnn1'  ## Options: a9a, ijcnn1\n",
    "loss = 'Regularised logistic loss'  ## Logistic loss, Regularised logistic loss\n",
    "repetitions = 3\n",
    "learning_rate = 0.5\n",
    "epochs = 30\n",
    "epsilon = 1e-4        ## Smallest possible loss variance\n",
    "minibatch_size = 128\n",
    "lambd = 0.001         ## Regularisation parameter of r(w)\n",
    "alpha = 1             ## Parameter of r(w)\n",
    "\n",
    "###############################\n",
    "M = [10,5,2,1,0.5,0.02]              ## array of all wanted M value tests\n",
    "###############################\n",
    "\n",
    "runs = Experiment.compare_M_Values(dataset_name, loss, repetitions, learning_rate, epochs, minibatch_size, M, lambd, alpha, opt)\n",
    "# Generate plots\n",
    "# Generate plots\n",
    "makePlots.plot_loss_vs_iterations(runs, epsilon, filename='M_Values_LvI.png')\n",
    "makePlots.plot_loss_vs_time(runs, epsilon, filename='M_Values_LvT.png')\n",
    "makePlots.plot_accuracy_vs_iterations(runs, filename='M_Values_AvI.png')\n",
    "makePlots.plot_accuracy_vs_time(runs, filename='M_Values_AvT.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2beadc",
   "metadata": {},
   "source": [
    "### Compare Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1501199",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'a9a'  ## Options: a9a, ijcnn1\n",
    "loss = 'Regularised logistic loss'  ## Logistic loss, Regularised logistic loss\n",
    "repetitions = 3\n",
    "epochs = 100\n",
    "epsilon = 1e-4        ## Smallest possible loss variance\n",
    "minibatch_size = 128\n",
    "M = 0.02              ## Upper bound for Hessian's Lipschitz constant\n",
    "lambd = 0.001         ## Regularisation parameter of r(w)\n",
    "alpha = 1             ## Parameter of r(w)\n",
    "\n",
    "\n",
    "#############################\n",
    "learning_rate = [1,0.75,0.5,0.25,0.1,0.05,0.01]         ## array of all wanted learning rates\n",
    "#############################\n",
    "\n",
    "runs = Experiment.compare_learningRates(dataset_name, loss, repetitions, learning_rate, epochs, minibatch_size, M, lambd, alpha, opt)\n",
    "# Generate plots\n",
    "# Generate plots\n",
    "makePlots.plot_loss_vs_iterations(runs, epsilon, filename='LearningRates_LvI.png')\n",
    "makePlots.plot_loss_vs_time(runs, epsilon, filename='LearningRates_LvT.png')\n",
    "makePlots.plot_accuracy_vs_iterations(runs, filename='LearningRates_AvI.png')\n",
    "makePlots.plot_accuracy_vs_time(runs, filename='LearningRates_AvT.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
